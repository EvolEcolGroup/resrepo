---
title: "workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{workflow}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
example_dir <- file.path(tempdir(),"resrepo_example")
unlink(example_dir,recursive = TRUE)
dir.create(example_dir, showWarnings = FALSE)
git2r::init(example_dir)
vignette_dir<-getwd()
knitr::opts_knit$set(root.dir = example_dir)
setwd(example_dir)

```

# resrepo: easy research on git

The aim of `resrepo` is to encourage and facilitate good practices when setting up and managing `git` repositories for scientific research projects. Scientific projects contain both code and data. `git` is designed to manage software code, but it is not suited to track large data files. There are extensions of `git`, such as `git-lfs` and `git-annex` that can handle data, but they can be complex to set up and difficult to use, especially when sharing your repository among collaborators. `resrepo` encourages good habits to manage your data alongside your code in plain `git`, ensuring reproducible science and a tidy repository that can used for publication of your project.

## Initialise the repository

Start by creating a blank repository, clone it on your computer, and use `resrepo` to initialise it. First, make sure that your working directory is set within the `git` repository:
```{r}
getwd()
```

We can now initialise the repository:
```{r}
library(resrepo)
init_resrepo()
```

Let us look at the content of our repository:

```{r}
fs::dir_tree()
```

In `resrespo`, we keep data, code, results, and the write-up separate. This 
allows for a tidy structure that can flexibly accommodate very complex projects.
Note that there are a number of README.md files, which provide instructions and advice
related to the files that go into that particular directory.

If you created the repository with `init_resrepo`, it would be a good idea to commit the changes, so that you have a clean repository template as your first commit, and you
can best document the changes that you make as you progress.

```{r echo=FALSE, results=FALSE}
git2r::add(path=".")
git2r::commit(message="initialise resrepo template", all=TRUE)
```

First, you should modify the main README.md, found at the root of the project,
to describe your project. That document will act as the landing page of your
git repository in GitHub/GitLab.

## Set up data directories

Next, you will need to bring some data into your repository. There are two main categories of data: "raw" data, which are primary data (e.g. measurements you made in the lab or the field, fastq files generated by a sequencer, remote sensing data downloaded from NASA); and "intermediate" data (data that you generated from raw, and that in turn will be used as the base of further analysis). In `resrepo`, we store data in one or more sub-directories within `/data/raw` or `/data/intermediate`, depending on their category. The template has a `/raw/default` subdir, which you could use if you have a simple project with relatively
few data files, but you are free to remove it and create alternative ones.

For each first level sub-directory within `/data/raw` and `/data/intermediate`, there should be an entry in the file `data_source_list.csv`. The file describes where the data can be found (e.g. directly tracked on git, Zenodo, OneDrive, etc.). For repeatable science, it is important that the provenance of data
is clear. For finished projects, you want to make sure that you use permanent repositories (e.g. Zenodo),
not your personal OneDrive! When a `resrepo` has just been initialised, we only have an entry for `/data/raw/default`:
```{r}
read.csv("./data/data_source_list.csv")
```

Note however that this entry is incomplete, as the `source` and `url` elements
are empty. By default, a `resrepo` does not track data files (as they are often too large for git), and each sub-directory should have a remote source that will allow other users to get the data needed
for the analysis. There should be one source for each first-level subdirectory (e.g. `/data/raw/defaults` or `/data/intermediate/rnaseq`); you are then free to further structure (or not) your data within multiple, higher level sub-directories within each first level sub-directory. If your datasets are small and stored as text files, you might decide to track them directly within your git repository. In that case, set the `source` and `url` as `git` in the `data_source_list.csv` table for that sub-directory, and modify `.gitignore` accordingly.

We can check that all entries are present and complete with:
```{r, error=TRUE}
data_source_check()
```

If we decided to mirror our data
on OneDrive, we can simply edit the information:

```{r}
data_source_edit(dir = "/data/raw/default",
                 source = "onedrive",
                 url = "https://my_one_drive/link_to_default_folder")
```

Note that you will need the link to the OneDrive folder where you will keep a
copy of the data. `resrepo` does not update the data files for you, so you
will have to manually keep the cloud source up-to-date (but see the
vignette on symbolic_links for a set up where mirroring is automatic).

If we inspect the data_source_list.csv file, we have:
```{r}
read.csv("./data/data_source_list.csv")
```

The notes are optional, but they allow you to add
additional information on where the data come from.

And now all checks will pass:
```{r}
data_source_check()
```
Now data sources have been arranged, commit changes for a clean repository.

```{r echo=FALSE, results="hide"}

git2r::add(path="./data")
git2r::commit(message="Added data sources", all=TRUE)

git2r::status()
```

No files should be stored directly in `/data/raw` or `/data/intermediate`, they should always be put in a first level sub-directory (otherwise we can not define the provenance in `data_source_list.csv`). If you decide to remove the `/raw/default` subdir and use a custom directory, make sure that the `data_source_list.csv` table is updated accordingly. If you are not generating any intermediate data, you can simply ignore the `/data/intermediate` directory.

## Adding some code and generating results

Git is designed to track code, and it does so elegantly out of the box. For code portability,
however, it is important to use relative paths in your scripts. So, from the `/code` directory, data
can be found as `../data/raw/default`. The other option is to use:
```{r}
data_dir <- path_resrepo("/data/raw/default")
data_dir
```

`path_resrespo` allows the use of paths relative to the root of the git
repository, irrespective of where the scripts are (e.g.
deep in a subdirectory of `code`).

It is also important that code saves data in the `results` directory. 
If you have many scripts, it is often better to have them each writing results into separate sub-directories, as it helps tracking where each result file comes from. So, for example,
`/code/s01_preprocessing.R` would write files into `/results/s01_preprocessing`. 
This strategy also allows to have a simple command at the beginning of a script which wipes the results directory before generating the new set of results (thus avoiding the risk of having old files from obsolete analyses floating around the repository).

An added challenge is posed by RMarkdown (Rmd) files, as they, by default, knit in the
same directory as the script (and thus would put the output into `code`). The function
`knit_to_results` changes this behaviour: when you click 'knit', the output will 
automatically be knitted to a folder under `results` with the same name as the `Rmd` file itself. 

To use the `knit_to_results` function, add it to the the YAML section of the 
RMarkdown document under the `knit` parameter. 

For example, if this is the YAML header of the RMarkdown file `code/s02_plots.Rmd` 
which is the 2nd step in an analysis workflow and produces some plots:

```
---
title: "Step 2: plots"
author: "Your name"
date: "2022-01-30"
knit: resrepo::knit_to_results
output: pdf_document
---
```
then the output of knitting this file will be a PDF document of the same name, in 
a `results` folder of the same name: `results/s02_plots/s02_plots.pdf`.

If the code in this Rmd file also saves plots to file (e.g. using `png()` or `ggsave()` functions),
you should ensure they are also written to the output folder `results/s02_plots`. This
way all the outputs from one Rmd file are in the same place, and can easily be linked back
to the script that produced them (`code/s02_plots.Rmd`).


## Writing it all up

Git is not really designed to handle Word or OpenOffice files. Ideally, text is 
kept in markdown files, but that format does not let itself nicely to formatting
for submission (e.g. bibliography from a reference manager). Having said that, 
you could, in principle, track Word/OpenOffice documents in the `writing` directory, 
as they are rarely large enough to cause trouble. If you are writing
collaboratively, you are more likely to use GoogleDoc or shared Word documents.
In that case, you can use the README.md in the `writing` directory to store the 
paths to those documents. By doing so, it means that a collaborator who has access 
to the repository can also find the manuscript easily (i.e.
everything is in one place).

<br>

***

<br>

# A step by step example

We will now illustrate how to populate a `resrespo` for a simple project. 

The first step is to download some data and put them in `/data/raw/default`. 
We will use some data from the package `palmerpenguins`, and copy them into our repository:
```{r}
data("penguins", package = "palmerpenguins")
write.csv(x = penguins,
          file = path_resrepo("/data/raw/default/penguins.csv"))
```

```{r echo=FALSE}
fs::dir_tree()
```

By default, the data are not tracked, so this file will not appear on your Git 
tab on the right hand side of RStudio. We can use a command from the `git2r` library
to check this (but you won't need it, you can just check the Git tab in Rstudio):

```{r}
git2r::status()
```

We will now process the data: we want to create a version of the dataset without NAs
(of course, you will normally have much more complex data processing). 


We will write a simple script to put in code, and we will call it `s01-preprocess_penguins.R`.
We want the script to save a new version of the dataset. This should go in `/raw/intermediate`,
in a new directory. It is good practice to use the same name as the script, so
that you can easily track down where the files come from.
We first need to create this directory:
```{r}
data_source_add(dir = "/data/intermediate/s01-preprocess_penguins",
                source = "onedrive",
                url = "https://my_one_drive/link_to_penguins_folder")
```

```{r echo=FALSE}
fs::dir_tree()
```
And that the source list is fully valid:
```{r}
data_source_check()
```

Let us copy in from the package a script to preprocess the data:

```{r echo=FALSE, results="hide"}
file.copy(system.file("template_extra/s01-preprocess_penguins.R",
                      package="resrepo"),
          path_resrepo("/code/s01-preprocess_penguins.R"))
```

Now that we have a directory where we can store our intermediate dataset, we can
create our script that will generate it. The script will include:
```{r echo=FALSE}
readLines(path_resrepo("/code/s01-preprocess_penguins.R"))
```

```{r echo=FALSE}
fs::dir_tree()
```

Let's now execute the script (if running R from the command line, 
you might use `Rscript`):

```{r}
source(path_resrepo("/code/s01-preprocess_penguins.R"))
```
And we can see that the resulting dataset was stored as expected in the intermediate data
directory.

```{r echo=FALSE}
fs::dir_tree()
```

The only files that should be visible to git are the source list table (which
was updated when we created a new data source) and the script, but not the data
files that we created (as we are ignoring the data directories):
```{r}
git2r::status()
```

We should now commit our changes to our git repository.
```{r echo=FALSE, results="hide"}

git2r::add(path=".")
git2r::commit(message="Added preprocess script", all=TRUE)
```

And check that after that, we have a clean repository:
```{r}
git2r::status()
```

For other parts of the analysis, a RMarkdown document might be more suitable
than a plain script. 

To create a new Rmd file from the `resrepo` template Rmd file, use the `create_rmd` 
function and specify the filename and which folder it should be in (you do not 
need to include the `.Rmd` file extension).
```{r}
create_rmd("code/s02-plots")
```

We can see that a new Rmd file has appeared.
```{r}
fs::dir_tree()
```

You can now open this file and edit it to do your analyses. For this vignette, 
we will replace it with a pre-written analysis file to continue the example.

We will use an Rmd file that generates a plot. 
```{r}
file.copy(from = system.file("template_extra/s02-plots.Rmd", package="resrepo"),
          to = path_resrepo("/code/s02-plots.Rmd"),
          overwrite = TRUE)
```

The Rmd file `s02-plots.Rmd` includes example code to read in the data:
```{r}
data_dir <- path_resrepo("/data/intermediate/s01-preprocess_penguins/")
penguins <- read.csv(file.path(data_dir,"penguins_na_omit.csv"))
```

And then to plot mass against flipper length whilst saving a pdf of the plot
in the appropriate `/results` subdirectory:

```{r plot_in_results, eval=FALSE}
# create directory to store the results
dir.create(path_resrepo('/results/s02-plots'), 
           showWarnings = FALSE)
res_dir <- path_resrepo('/results/s02-plots')
# create plot from intermediate data
penguins_plot <- plot(x = penguins_na_omit$body_mass_g, 
                      y = penguins_na_omit$flipper_length_mm
                      )
# and save it
png(file.path(res_dir, "penguins_plot.png"))
print(penguins_plot)
dev.off()

```

We can now knit that Rmd. You can do that from RStudio by clicking the Knit button.
```{r, echo=F, results='hide'}
knit_to_results(path_resrepo("/code/s02-plots.Rmd"))

```

We can see that our PDF and plot were saved in the right place.
```{r}
fs::dir_tree()
```
`results` are tracked by default:
```{r}
git2r::status()
```

Now we just need to commit our changes, and we have a full repository with code
and results, and a traceable source of data. Before we publish, we would move
those data directories to something like Zenodo rather than using OneDrive/Dropbox.
```{r echo=FALSE, results="hide"}

git2r::add(path=".")
git2r::commit(message="Save plot", all=TRUE)

git2r::status()
```


## FAQ

How do I decide whether to put certain files in `data/intermediate` or `results`?

In principle, it does not matter that much where you put them, as long as they are not
too large. Sometimes the same file can be both a result and data. However, for very large
files, which you don't want to track directly in git, it is easiest to put them under
`data/intermediate` as `data` are not tracked by default.


### NOTES to integrate into the above

**Naming files and folders**
Never have spaces in your directory or file names (underscore can be used).
Please, only use lowercase letters when naming files and folders. 
Avoid naming your files starting with a number (e.g. instead of “01_process_data.R” use “s01_process_data.R”)
Do not use generic names such as “Figure_01”, “Figure_02”, etc. for your files and folders. Use more descriptive names as numbers change. It will make your life easier when you will need to work again on the project.
Naming your results folder as the script that created it (easy to find it after)  

**File size**
Small data (<1MB or so) can be kept on GitHub. For all the other data/files please use an alternate source (Google Drive, OneDrive, DropBox, etc.). We will work on scripts to retrieve data from different sources.  


**This are git specific advice, we could create a separate document with some git tips.**

**Commit**
Think carefully about your commit messages and branch names, as they will be very useful for returning to past changes for both you and others (for example when a project is made publicly available). Please make them informative.  

**Merge**
Once you merge a branch into master, kill the branch to avoid problems in the future.
Do not name a branch with the name of a branch that has been merged/deleted recently, as this may create problems. 

**Binary files**
Do not upload binary data (e.g.*.rds): changes in such files are too heavy to be handled by GitHub.  

## Github setup
If you already have a ssh key, follow these steps:
https://docs.github.com/en/authentication/connecting-to-github-with-ssh/checking-for-existing-ssh-keys 

If you need to generate a new key and add it to GitHub:
**1. Generate ssh key**
Source: https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent

**2. add key to account**
Source: https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account  

**3. Test connection**
Source: https://docs.github.com/cn/authentication/connecting-to-github-with-ssh/testing-your-ssh-connection  

### Worked example (branch)  
**Summary**

