---
title: "workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
example_dir <- file.path(tempdir(),"resrepo_example")
unlink(example_dir,recursive = TRUE)
dir.create(example_dir, showWarnings = FALSE)
git2r::init(example_dir)
vignette_dir<-getwd()
knitr::opts_knit$set(root.dir = example_dir)

```

# resrepo: easy research on git

The aim of `resrepo` is to encourage and facilitate good practices when setting up and managing `git` repositories for scientific research projects. Scientific projects contain both code and data. `git` is designed to manage software code, but it is not suited to track large data files. There are extensions of `git`, such as `git-lfs` and `git-annex` that can handle data, but they can be complex to set up and difficult to use, especially when sharing your repository among collaborators. `resrepo` encourages good habits to manage your data alongside your code in plain `git`, ensuring reproducible science and a tidy repository that can used for publication of your project.

## Initialise the repository

Start by creating a repository for your project using the `resrepo_template` repository as a template. Within the EvolEcolGroup GitHub, if you click `New` to create a new project repository you will be offered the chance to use a `Repository template; Start your repository with a template repository's contents`. Here there is a dropdown box where you can select this template `resrepo_template`.

Alternatively, you can create a blank repository, clone it on your computer, and use `resrepo` to initialise it. First, make sure that your working directory is set within the `git` repository:
```{r}
getwd()
```

We can now initialise the repository:
```{r}
library(resrepo)
init_resrepo()
```

Let us look at the content of our repository:

```{r}
fs::dir_tree()
```

In `resrespo`, we keep data, code, results, and the write-up separate. This 
allows for a tidy structure that can flexibly accommodate very complex projects.
Note that there are a number of README.md files, which provide instructions and advice
related to the files that go into that particular directory.

If you created the repository with `init_resrepo`, it would be a good idea to commit the changes, so that you have a clean repository template as your first commit, and you
can best document the changes that you make as you progress.

```{r echo=FALSE, results=FALSE}
git2r::add(path=".")
git2r::commit(message="initialise resrepo template", all=TRUE)
```

First, you should modify the main README.md, found at the root of the project,
to describe your project. That document will act as the landing page of your
git repository in GitHub/GitLab.

## Set up data directories

Next, you will need to bring some data into your repository. There are two main categories of data: "raw" data, which are primary data (e.g. measurements you made in the lab or the field, fastq files generated by a sequencer, remote sensing data downloaded from NASA); and "intermediate" data (data that you generated from raw, and that in turn will be used as the base of further analysis). In `resrepo`, we store data in one or more sub-directories within `/data/raw` or `/data/intermediate`, depending on their category. The template has a `/raw/default` subdir, which you could use if you have a simple project with relatively
few data files, but you are free to remove it and create alternative ones.

For each first level sub-directory within `/data/raw` and `/data/intermediate`, there should be an entry in the file `data_source_list.csv`. The file describes where the data can be found (e.g. directly tracked on git, Zenodo, OneDrive, etc.). For repeatable science, it is important that the provenance of data
is clear. For finished projects, you want to make sure that you use permanent repositories (e.g. Zenodo),
not your personal OneDrive! When a `resrepo` has just been initialised, we only have an entry for `/data/raw/default`:
```{r}
read.csv("./data/data_source_list.csv")
```

Note however that this entry is incomplete, as the `source` and `url` elements
are empty. By default, a `resrepo` does not track data files (as they are often too large for git), and each sub-directory should have a remote source that will allow other users to get the data needed
for the analysis. There should be one source for each first-level subdirectory (e.g. `/data/raw/defaults` or `/data/intermediate/rnaseq`); you are then free to further structure (or not) your data within multiple, higher level sub-directories within each first level sub-directory. If your datasets are small and stored as text files, you might decide to track them directly within your git repository. In that case, set the `source` and `url` as `git` in the `data_source_list.csv` table for that sub-directory, and modify `.gitignore` accordingly.

We can check that all entries are present and complete with:
```{r, error=TRUE}
data_source_check()
```

If we decided to mirror our data
on OneDrive, we can simply edit the information:

```{r}
data_source_edit(dir = "/data/raw/default",
                 source = "onedrive",
                 url = "https://my_one_drive/link_to_default_folder")
```

Note that you will need the link to the OneDrive folder where you will keep a
copy of the data. `resrepo` does not update the data files for you, so you
will have to manually keep the cloud source up-to-date (but see the
vignette on symbolic_links for a set up where mirroring is automatic).

If we inspect the data_source_list.csv file, we have:
```{r}
read.csv("./data/data_source_list.csv")
```

The notes are optional, but they allow you to add
additional information on where the data come from.

And now all checks will pass:
```{r}
data_source_check()
```
Now data sources have been arranged, commit changes for a clean repository.

```{r echo=FALSE, results="hide"}

git2r::add(path="./data")
git2r::commit(message="Added data sources", all=TRUE)

git2r::status()
```

No files should be stored directly in `/data/raw` or `/data/intermediate`, they should always be put in a first level sub-directory (otherwise we can not define the provenance in `data_source_list.csv`). If you decide to remove the `/raw/default` subdir and use a custom directory, make sure that the `data_source_list.csv` table is updated accordingly. If you are not generating any intermediate data, you can simply ignore the `/data/intermediate` directory.

## Adding some code and generating results

Git is designed to track code, and it does so elegantly out of the box. For code portability,
however, it is important to use relative paths in your scripts. So, from the `/code` directory, data
can be found as `../data/raw/default`. The other option is to use:
```{r}
default_dir <- path_resrepo("/data/raw/default")
default_dir
```

`path_resrespo` allows the use of paths relative to the root of the git
repository, irrespective of where the scripts are (e.g.
deep in a subdirectory of `code`).

It is also important that code saves data in the `results` directory. 
If you have many scripts, it is often better to have them each writing results into separate sub-directories, as it helps tracking where each result file comes from. So, for example,
`/code/preprocessing.R` would write files into `/results/preprocessing`. 
This strategy also allows to have a simple command at the beginning of a script which wipes the results directory before generating the new set of results (thus avoiding the risk of having old files from obsolete analyses floating around the repository).

An added challenge is posed by RMarkdown files, as they, by default, knit in the
same directory as the script (and thus would put results into `code`). The function
`knit_to_results` changes this behaviour. You have to add it to the the YAML section
of the RMarkdown document.

### ADD EXAMPLE YAML TO DO!

## Writing it all up
Git is not really designed to handle Word or OpenOffice files. Ideally, text is 
kept in markdown files, but that format does not let itself nicely to formatting
for submission (e.g. bibliogrpahy from a reference manager). Having said that, 
you could, in principle, track Word/Openoffice documents in the `writing` directory, as they are rarely large enough to cause trouble. If you are writing
collaboratively, you are more likely to use GoogleDoc or shared Word documents.
In that case, you can use the README.md in the `writing` directory to store the paths to those documents. By doing
so, it means that a collaborator who has access to the repository can also find the manuscript easily (i.e.
everything is in one place).

# A step by step example
We will now illustrate how to populate a `resrespo` for a simple project.
The first step is to download some data and put them in `/data/raw/default`. We will use some data from the package `palmerpenguins`, and copy them into our repository:
```{r}
data("penguins", package = "palmerpenguins")
write.csv(x = penguins,
          file = path_resrepo("/data/raw/default/penguins.csv"))
```

```{r echo=FALSE}
fs::dir_tree()
```

By default, the data are not tracked, so this file will not appear on your Git 
tab on the right hand side of RStudio. We can use a command from the `git2r` library
to check this (but you won't need it, you can just check the Git tab in Rstudio):

```{r}
git2r::status()
```

We will now process the data: we want to create a version of the dataset without NAs
(of course, you will normally have much more complex data processing). We will
write a simple script to put in code, and we will call it `s01-preprocess_penguins.R`.
We want the script to save a new version of the dataset. This should go in `/raw/intermediate`,
in a new directory. We first need to create this directory:
```{r}
data_source_add(dir = "/data/intermediate/penguins",
                source = "onedrive",
                url = "https://my_one_drive/link_to_penguins_folder")
```

```{r echo=FALSE}
fs::dir_tree()
```

Now that we have a directory where we can store our intermediate dataset, we can
create our script, which will include:
```{r process_raw_data}
penguins <- read.csv(path_resrepo("/data/raw/default/penguins.csv"))
penguins_na_omit <- na.omit(penguins)
write.csv(penguins_na_omit, 
          file = path_resrepo("/data/intermediate/penguins/penguins_na_omit.csv"),
          row.names = FALSE)                          
```

We can confirm that the file is now in the right place:
```{r echo=FALSE}
fs::dir_tree()
```
And that the source list is fully valid:
```{r}
data_source_check()
```

The only file that should be tracked by git is the source list table, which
has been updated, but not the data files that we created:
```{r}
git2r::status()

```
Read in intermediate data file that we created earlier.

```{r read in data}
#read in preprocessed data from intermediate data directory
penguins_na_omit <- read.csv(path_resrepo("/data/intermediate/penguins/penguins_na_omit.csv"))
```

Plot body mass against flipper length.

```{r make plot}
#create plot from intermediate data
penguins_plot <- plot(x = penguins_na_omit$body_mass_g, 
                      y = penguins_na_omit$flipper_length_mm
                      #, col= penguins_na_omit$species
                      )

dir.create(path_resrepo("/results/penguins"))
pdf(path_resrepo("/results/penguins/penguins_plot.pdf"))

print(penguins_plot)

dev.off()

```

Now commit plot and check git status.
```{r}

git2r::add(path="./results/penguins")
git2r::commit(message="Save plot", all=TRUE)

git2r::status()
```



## FAQ

How do I decide whether to put certain files in `data/intermediate` or `results`?

In principle, it does not matter that much where you put them, as long as they are not
too large. Sometimes the same file can be both a result and data. However, for very large
files, which you don't want to track directly in git, it is easiest to put them under
`data/intermediate` as `data` are not tracked by default.


### NOTES to modify

**Naming files and folders**
Never have spaces in your directory or file names (underscore can be used).
Please, only use lowercase letters when naming files and folders. 
Avoid naming your files starting with a number (e.g. instead of “01_process_data.R” use “s01_process_data.R”)
Do not use generic names such as “Figure_01”, “Figure_02”, etc. for your files and folders. Use more descriptive names as numbers change. It will make your life easier when you will need to work again on the project.
Naming your results folder as the script that created it (easy to find it after)  

**File size**
Small data (<1MB or so) can be kept on GitHub. For all the other data/files please use an alternate source (Google Drive, OneDrive, DropBox, etc.). We will work on scripts to retrieve data from different sources.  

**Commit**
Think carefully about your commit messages and branch names, as they will be very useful for returning to past changes for both you and others (for example when a project is made publicly available). Please make them informative.  

**Merge**
Once you merge a branch into master, kill the branch to avoid problems in the future.
Do not name a branch with the name of a branch that has been merged/deleted recently, as this may create problems. 

**Binary files**
Do not upload binary data (e.g.*.rds): changes in such files are too heavy to be handled by GitHub.  

## Github setup
If you already have a ssh key, follow these steps:
https://docs.github.com/en/authentication/connecting-to-github-with-ssh/checking-for-existing-ssh-keys 

If you need to generate a new key and add it to GitHub:
**1. Generate ssh key**
Source: https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent

**2. add key to account**
Source: https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account  

**3. Test connection**
Source: https://docs.github.com/cn/authentication/connecting-to-github-with-ssh/testing-your-ssh-connection  

### Worked example (branch)  
**Summary**

